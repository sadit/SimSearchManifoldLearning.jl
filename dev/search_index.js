var documenterSearchIndex = {"docs":
[{"location":"apiml/","page":"API ManifoldLearning","title":"API ManifoldLearning","text":"\nCurrentModule = SimSearchManifoldLearning\nDocTestSetup = quote\n    using SimSearchManifoldLearning\nend","category":"page"},{"location":"apiml/#SimilaritySearch-and-ManifoldLearning","page":"API ManifoldLearning","title":"SimilaritySearch and ManifoldLearning","text":"","category":"section"},{"location":"apiml/","page":"API ManifoldLearning","title":"API ManifoldLearning","text":"ManifoldKnnIndex","category":"page"},{"location":"apiml/#SimSearchManifoldLearning.ManifoldKnnIndex","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ManifoldKnnIndex","text":"ManifoldKnnIndex{DistType,MinRecall}\n\nImplements the ManifoldLearning.AbstractNearestNeighbors interface to interoperate with the non-linear dimensionality reduction methods of the ManifoldLearning package.\n\nIt should be passed to the fit method as a type, e.g.,\n\nfit(ManifoldKnnIndex{L2Distance,0.9})  # will use an approximate index with an expected recall of 0.9\n\nDistType should be any in SimilaritySearch package, or Distances.jl or any other following the SemiMetric.\n\nThe second argument of the composite type indicates the quality and therefore the type of index to use:\n\nIt takes values between 0 and 1.\n0 means for a SearchGraph index using ParetoRecall optimization for the construction and searching, this will try to achieve a competitive structure in both quality and search speed\n1 means for a ExhaustiveSearch index, this will compute the exact solution (exact knns) but at cost speed. Can work pretty well on small datasets and very high dimensionality. Really high dimensions suffer from the curse of dimensionality such that an index like SearchGraph degrades to ExhaustiveSearch.\n0 < value < 1: Uses a SearchGraph and is the minimum recall-score quality that the index should perform. In particular, it constructs the index using ParetoRecall and the use a final optimization with MinRecall. It takes values from 0 to 1, small values produce faster searches with lower qualities and high values slower searches with higher quality. Values 0.8 or 0.9 should work pretty well.\n\nNote: The minimum performance is evaluated in a small training set took from the database, this could yield to some kind of overfitting in the parameters, and therefore, perform not so good in an unseen query set. If you note this effect, please see SimilaritySearch documentation function optimize!.\n\n\n\n\n\n","category":"type"},{"location":"apiml/","page":"API ManifoldLearning","title":"API ManifoldLearning","text":"The distance functions are defined to work under the evaluate(::SemiMetric, u, v) function (borrowed from Distances.jl package).","category":"page"},{"location":"apiml/#KNN-predefined-types","page":"API ManifoldLearning","title":"KNN predefined types","text":"","category":"section"},{"location":"apiml/","page":"API ManifoldLearning","title":"API ManifoldLearning","text":"ExactEuclidean\nExactManhattan\nExactChebyshev\nExactCosine\nExactAngle\nApproxEuclidean\nApproxManhattan\nApproxChebyshev\nApproxCosine\nApproxAngle","category":"page"},{"location":"apiml/#SimSearchManifoldLearning.ExactEuclidean","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ExactEuclidean","text":"ExactEuclidean\n\nManifoldKnnIndex's type specialization for exact search with the Euclidean distance.\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ExactManhattan","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ExactManhattan","text":"ExactManhattan\n\nManifoldKnnIndex's type specialization for exact search with the Manhattan distance.\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ExactChebyshev","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ExactChebyshev","text":"ExactChebyshev\n\nManifoldKnnIndex's type specialization for exact search with the Chebyshev distance.\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ExactCosine","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ExactCosine","text":"ExactCosine\n\nManifoldKnnIndex's type specialization for exact search with the cosine distance.\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ExactAngle","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ExactAngle","text":"ExactAngle\n\nManifoldKnnIndex's type specialization for exact search with the angle distance.\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ApproxEuclidean","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ApproxEuclidean","text":"ApproxEuclidean\n\nManifoldKnnIndex's type specialization for approximate search with the Euclidean distance (expected recall of 0.9)\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ApproxManhattan","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ApproxManhattan","text":"ApproxManhattan\n\nManifoldKnnIndex's type specialization for approximate search with the Manhattan distance (expected recall of 0.9)\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ApproxChebyshev","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ApproxChebyshev","text":"ApproxChebyshev\n\nManifoldKnnIndex's type specialization for approximate search with the Chebyshev distance (expected recall of 0.9)\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ApproxCosine","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ApproxCosine","text":"ApproxCosine\n\nManifoldKnnIndex's type specialization for approximate search with the Cosine distance (expected recall of 0.9)\n\n\n\n\n\n","category":"type"},{"location":"apiml/#SimSearchManifoldLearning.ApproxAngle","page":"API ManifoldLearning","title":"SimSearchManifoldLearning.ApproxAngle","text":"ApproxAngle\n\nManifoldKnnIndex's type specialization for approximate search with the angle distance (expected recall of 0.9)\n\n\n\n\n\n","category":"type"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"\nCurrentModule = SimSearchManifoldLearning\nDocTestSetup = quote\n    using SimSearchManifoldLearning\nend","category":"page"},{"location":"apiumap/#UMAP","page":"UMAP","title":"UMAP","text":"","category":"section"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"UMAP\nfit\npredict\noptimize_embeddings!","category":"page"},{"location":"apiumap/#SimSearchManifoldLearning.UMAP","page":"UMAP","title":"SimSearchManifoldLearning.UMAP","text":"struct UMAP\n\nThe UMAP model struct\n\nProperties\n\ngraph: The fuzzy simplicial set that represents the all knn graph\nembedding: The embedding projection \nk the number of neighbors used to create the model\na and b: parameters to ensure an well distributed and smooth projection (from min_dist and spread arguments in fit)\nindex: the search index, it can be nothing if the model is handled directly with precomputed knns and dists matrices\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#StatsAPI.fit","page":"UMAP","title":"StatsAPI.fit","text":"fit(Type{UMAP}, knns, dists; <kwargs>) -> UMAP object\n\nCreate a model representing the embedding of data (X, dist) into maxoutdim-dimensional space. Note that knns and dists jointly specify the all k nearest neighbors of (X dist), these results must not include self-references. See the allknn method in SimilaritySearch.\n\nArguments\n\nknns: A (k n) matrix of integers (identifiers).\ndists: A (k n) matrix of floating points (distances).\n\nIt uses all available threads for the projection.\n\nKeyword Arguments\n\nmaxoutdim::Integer=2: The number of components in the embedding\nn_epochs::Integer = 300: the number of training epochs for embedding optimization\nlearning_rate::Real = 1: the initial learning rate during optimization\nlearning_rate_decay::Real = 0.9: how much learning_rate is updated on each epoch (learning_rate *= learning_rate_decay) (a minimum value is also considered as 1e-6)\nlayout::AbstractLayout = SpectralLayout(): how to initialize the output embedding\nmin_dist::Real = 0.1: the minimum spacing of points in the output embedding\nspread::Real = 1: the effective scale of embedded points. Determines how clustered embedded points are in combination with min_dist.\nset_operation_ratio::Real = 1: interpolates between fuzzy set union and fuzzy set intersection when constructing the UMAP graph (global fuzzy simplicial set). The value of this parameter should be between 1.0 and 0.0: 1.0 indicates pure fuzzy union, while 0.0 indicates pure fuzzy intersection.\nlocal_connectivity::Integer = 1: the number of nearest neighbors that should be assumed to be locally connected. The higher this value, the more connected the manifold becomes. This should not be set higher than the intrinsic dimension of the manifold.\nrepulsion_strength::Real = 1: the weighting of negative samples during the optimization process.\nneg_sample_rate::Integer = 5: the number of negative samples to select for each positive sample. Higher values will increase computational cost but result in slightly more accuracy.\na = nothing: this controls the embedding. By default, this is determined automatically by min_dist and spread.\nb = nothing: this controls the embedding. By default, this is determined automatically by min_dist and spread.\n\n\n\n\n\nfit(::Type{<:UMAP}, index_or_data;\n    k=15,\n    dist::SemiMetric=L2Distance,\n    kwargs...)\n\nWrapper for fit that computes n_nearests nearest neighbors on index_or_data and passes these and kwargs to regular fit.\n\nArguments\n\nindex_or_data: an already constructed index (see SimilaritySearch), a matrix, or an abstact database (SimilaritySearch)\nk=15: number of neighbors to compute\ndist=L2Distance(): A distance function (see Distances.jl)\n\n\n\n\n\nfit(UMAP::UMAP, maxoutdim; <kwargs>)\n\nReuses a previously computed model with a different number of components\n\nKeyword arguments\n\nn_epochs=50: number of epochs to run\nlearning_rate::Real = 1f0: initial learning rate\nlearning_rate_decay::Real = 0.9f0: how learning rate is adjusted per epoch learning_rate *= learning_rate_decay\nrepulsion_strength::Float32 = 1f0: repulsion force (for negative sampling)\nneg_sample_rate::Integer = 5: how many negative examples per object are used.\n\n\n\n\n\n","category":"function"},{"location":"apiumap/#StatsAPI.predict","page":"UMAP","title":"StatsAPI.predict","text":"predict(model::UMAP)\n\nReturns the internal embedding (the entire dataset projection)\n\n\n\n\n\npredict(model::UMAP, Q::AbstractDatabase; k::Integer=15, kwargs...)\npredict(model::UMAP, knns, dists; <kwargs>) -> embedding\n\nUse the given model to embed new points Q into an existing embedding produced by (X dist). The second function represent Q using its k nearest neighbors in X under some distance function (knns and dists) See searchbatch in SimilaritySearch to compute both (also for AbstractDatabase objects).\n\nArguments\n\nmodel: The fitted model\nknns: matrix of identifiers (integers) of size (k Q)\ndists: matrix of distances (floating point values) of size (k Q)\n\nNote: the number of neighbors k (embedded into knn matrices) control the embedding. Larger values capture more global structure in the data, while small values capture more local structure.\n\nKeyword Arguments\n\nn_epochs::Integer = 30: the number of training epochs for embedding optimization\nlearning_rate::Real = 1: the initial learning rate during optimization\nlearning_rate_decay::Real = 0.8: A decay factor for the learning_rate param (on each epoch)\nset_operation_ratio::Real = 1: interpolates between fuzzy set union and fuzzy set intersection when constructing the UMAP graph (global fuzzy simplicial set). The value of this parameter should be between 1.0 and 0.0: 1.0 indicates pure fuzzy union, while 0.0 indicates pure fuzzy intersection.\nlocal_connectivity::Integer = 1: the number of nearest neighbors that should be assumed to be locally connected. The higher this value, the more connected the manifold becomes. This should not be set higher than the intrinsic dimension of the manifold.\nrepulsion_strength::Real = 1: the weighting of negative samples during the optimization process.\nneg_sample_rate::Integer = 5: the number of negative samples to select for each positive sample. Higher values will increase computational cost but result in slightly more accuracy.\n\n\n\n\n\n","category":"function"},{"location":"apiumap/#Layouts","page":"UMAP","title":"Layouts","text":"","category":"section"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"RandomLayout\nSpectralLayout\nPrecomputedLayout\nKnnGraphLayout","category":"page"},{"location":"apiumap/#SimSearchManifoldLearning.RandomLayout","page":"UMAP","title":"SimSearchManifoldLearning.RandomLayout","text":"RandomLayout <: AbstractLayout\n\nInitializes the embedding using a random set of points. It may converge slowly\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#SimSearchManifoldLearning.SpectralLayout","page":"UMAP","title":"SimSearchManifoldLearning.SpectralLayout","text":"SpectralLayout <: AbstractLayout\n\nInitializes the embedding using the spectral layout method. It could be costly in very large datasets.\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#SimSearchManifoldLearning.PrecomputedLayout","page":"UMAP","title":"SimSearchManifoldLearning.PrecomputedLayout","text":"PrecomputedLayout <: AbstractLayout\n\nInitializes the embedding using a previously computed layout, i.e., (maxoutdim, n_points) matrix. \n\n\n\n\n\n","category":"type"},{"location":"apiumap/#SimSearchManifoldLearning.KnnGraphLayout","page":"UMAP","title":"SimSearchManifoldLearning.KnnGraphLayout","text":"KnnGraphLayout <: AbstractLayout\n\nA lattice like + clouds of points initialization that uses the computed all-knn graph. This layout initialization is a simple proof of concept, so please use it under this assumption.\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#Precomputed-Knn-matrices","page":"UMAP","title":"Precomputed Knn matrices","text":"","category":"section"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"If you don't want to use SimilaritySearch for solving k nearest neighbors, you can also pass precomputed knns and distances matrices.","category":"page"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"PrecomputedKnns\nPrecomputedAffinityMatrix","category":"page"},{"location":"apiumap/#SimSearchManifoldLearning.PrecomputedKnns","page":"UMAP","title":"SimSearchManifoldLearning.PrecomputedKnns","text":"struct PrecomputedKnns <: AbstractSearchContext\n    knns\n    dists\nend\n\nAn index-like wrapper for precomputed all-knns (as knns and dists matrices (k, n))\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#SimSearchManifoldLearning.PrecomputedAffinityMatrix","page":"UMAP","title":"SimSearchManifoldLearning.PrecomputedAffinityMatrix","text":"struct PrecomputedAffinityMatrix <: AbstractSearchContext\n    dists # precomputed distances for all pairs (squared matrix)\nend\n\nAn index-like wrapper for precomputed affinity matrix.\n\n\n\n\n\n","category":"type"},{"location":"apiumap/#Distance-functions","page":"UMAP","title":"Distance functions","text":"","category":"section"},{"location":"apiumap/","page":"UMAP","title":"UMAP","text":"The distance functions are defined to work under the evaluate(::SemiMetric, u, v) function (borrowed from Distances.jl package).","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = SimSearchManifoldLearning","category":"page"},{"location":"#Using-SimilaritySearch-for-ManifoldLearning","page":"Home","title":"Using SimilaritySearch for ManifoldLearning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"One of the main component of non-linear projection methods is the solution of large batches of k nearest neighbors queries. In this sense SimilaritySearch provides fast multithreaded algorithms using exact and approximated searching algorihtms. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides two main structs UMAP, and ManifoldKnnIndex (to work with ManifoldLearning)","category":"page"},{"location":"#UMAP","page":"Home","title":"UMAP","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package provides a pure Julia implementation of the Uniform Manifold Approximation and Projection dimension reduction algorithm","category":"page"},{"location":"","page":"Home","title":"Home","text":"McInnes, L, Healy, J, Melville, J, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. ArXiV 1802.03426, 2018","category":"page"},{"location":"","page":"Home","title":"Home","text":"The implementation in this package is based on the UMAP.jl package by Dillon Gene Daudert and collaborators. Forked and adapted to work with SimilaritySearch and take advantage of multithreading systems and different layout initializations. It can use any distance function from SimilaritySearch, Distances.jl, StringDistances.jl, or any distance function implemented by the user.","category":"page"},{"location":"","page":"Home","title":"Home","text":"SimSearchManifoldLearning provides an implementation that partially supports the ManifoldLearning API using fit and predict, and similar arguments.","category":"page"},{"location":"#ManifoldLearning-and-ManifoldKnnIndex","page":"Home","title":"ManifoldLearning and ManifoldKnnIndex","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We implemented the k nearest neighbor solver API of ManifoldLearning such that SimilaritySearch can be used. The use of SimilaritySearch gives multithreading k nearest neighbors solution, different distance functions and the different treadoffs between quality and speed for large datasets for the k nearest neighbors queries.","category":"page"},{"location":"#Examples","page":"Home","title":"Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Some examples are already working in SimilaritySearchDemos.","category":"page"}]
}
